## 19 июня 2023 - AGI-23 | Interpretable Natural Language Processing (INLP)
[![Watch the video](https://img.youtube.com/vi/FB6g0cuZpFc/hqdefault.jpg)](https://youtu.be/FB6g0cuZpFc)

S08 [00:04:45]  : Welcome to the final session of AGI 23. So we're day four, final workshop session. All things have consolidated to this moment right here, the only session running. So we'll have everybody together as we end on a high note. So this is the Interpretable Natural Language Processing, INLP, led by Anton Koloman. And we're going to start with the presentation by Patrick Hammer on NARS GPT with the demo. Welcome, Patrick. 

S02 [00:05:24]  : Thank you. 

S04 [00:05:30]  : So I will talk about how to combine our system, non-axiomatic reasoning system with GPT models. As you all know, there has been a huge progress with this large language models in terms of handling natural language in a computer system. And for us, there was always the question, how can we utilize this technology so that it's easier for people to interact with our reasoning system? That's one aspect. Of course, from a research perspective, there's also this aspect of, can we make LLMs reasons by themselves? Or can we potentially, can we get synergies from combining large language models with a reasoning system like ours. And I think that's something we as HEA researchers have to find out. And we essentially, especially focused at our system and tried to overcome some of the limitations which you see in large language models, such as limited contextual window, so that essentially, ideally, we would like to have long term memory and a system which can reason on reason in a long term manner. But I also want to mention the sponsorship of our research, especially Hugo Latope from Cisco, who has supported this research and contributed with us on this system. And also, Jurijs Pframer from Fraunhofer, who did a project combining us with neural networks, which has essentially converged into this LLM plus NAS. ideal. So let's dive into this overall. We will just briefly look at NAS. We will look at implementation of NAS for applications. I will only briefly mention it since you have, many of you have already watched the workshops, so not have so much redundant content. And we will quickly, we will look at the overall architecture of the system and also the mentioned synergies. What can we actually gain out of combining LLMs with this reasoning system? What can we get in terms of capabilities which are relevant? So, So as I mentioned, we want AI systems which we can naturally communicate with, but also we want these AI systems to remember information for a longer time period so that they don't forget what we tell them and that they can continue reasoning on that. And so there is this, how to say, there was a recent development, which is also relevant for this endeavor, which is vector databases that you can essentially calculate embeddings of sentences and then store the sentence with a vector embedding into a vector database. And then later, if you have a similar sentence, you can actually query for sentences which are close according to this embedding vector. Something we also have utilized, but we will see this in more detail. So overall, we try to overcome the limitations in terms of long-term memory. And we also want to feature this kind of reasoning capability, which NAS is particularly very reliable for. Reasoning is something which just works in a reasoning system since it's designed in. It's not a matter of making 40 inferences. It will not happen in a reasoning system. So we want to make use of this kind of capability in an effective way. That's the idea. Overall, our reasoning system, just very briefly, We focus on having a system which keeps adapting while it's running. So this fits perfectly into this picture. We want an AI system which can learn at runtime and remember the information. So this is perfectly compatible. What's also good in this particular integration between NAS and GPT, you also get some synergy in terms of being able to use the sensory motor functionalities of NAS while being able to communicate with the system in natural language. So there's also some synergy here that you can essentially make use of the functionality which the reasoning system is good at while having this natural way to communicate with it. So I will not dive too much into the reasoning itself, since many of you have already seen several repetitions in various talks. So we have this real-time reasoning system, which uses non-axiomatic logic for internal representation. So also in this NASH GPT project, what we essentially do is we let LLM translate English into this representation, into this representation so that the reasoning system can reason on it. In terms of evidence measure, there is something which is quite convenient. In NASA, we have this notion of positive and negative evidence. It's not a big deal if users over time input contradictory information. They might have some different beliefs about a certain subject, whether climate change, how serious the condition on this is, and there might be conflicting beliefs between different papers or different users might input different information, which will sometimes contradict each other. And with this reasoning system, you will have still a consistent belief base in the sense that you have this different pieces of evidence, some positive, some negative about a certain statement. Some people claim a certain thing is the case, another person claims it's not. So the system will essentially revise this pieces of information. And that's a powerful feature because it also allows you to correct the system's beliefs by providing additional evidence to the system. I will not dive into the implementation here too much. Most are already familiar with this implementation, C implementation of the system, which is particularly made to run efficiently. So it's an efficient implementation. We will not dive into this. We can skip. We will instead go into the approaches of how NLM can be combined with GPT, with reasoning. Of course, there is this approach to just train larger and larger, larger language models. But in terms of reasoning performance and factual Q&A performance, you see they are still not that good. They still make a lot of mistakes. So there's this question of how to bring reasoning into the picture. And in this particular project, we have essentially extended on this idea. If you take a vanilla GPT, you essentially enter natural language sentences and natural language questions, and it will spit out documents, essentially. So what we do is we will now incrementally go beyond that. So what you see in this picture is this idea of vector database attached. Like you input sentences. Sorry, you calculate the embedding of the sentences the user enters, and you store the embedding together with the sentence in the embedding database. And then when the user asks a question, The embedding of the question is calculated so that similar items from the embedding database will be pulled out from this long-term memory so that it will be available in the Bronc for GPT to answer the question. So this is like the like a simple way to add long-term memory to GPT but it will not lead to the system forming a consistent base of beliefs because if you have contradicting sentences in there either you get a random answer then Well, it will say it's not true because there's contradictory information. But it will not do this kind of evidence accumulation, which is really necessary to make the system form consistent set of beliefs and answer accordingly. So we extend this picture here with taking, putting us into the picture by essentially now extracting relations from sentences. So if you have a sentence like Garfield, it's a mouse in the garden. The idea is you let GPT decompose this into smaller relationships, like Garfield is eating a mouse, Garfield is in the garden. So you decompose it into smaller relations, and these relations are easily translated into NAS because it's essentially just subject-object triplets. So that's about that. And now the interesting thing is when NAS makes derivations, we also It's also subject, object, verb. It's also relational representations, essentially, which we translate back into simple natural language, which is always easy. If you have a formal language, it's easy to just translate it in a simple natural language. That's just the opposite way, which is really difficult to go from natural language and its complexities to formal language. So here, that's just when the system makes derivations. And I can use my mouse pointer here. When the system makes derivations, we create a simple sentence representation, calculate the embedding of it. So essentially, now you have in this embedding storage not just the input information of the user. You also have information which was derived by the reasoning system. And that's really the key here, not just input information from the user, but also the raft results are now available here. So when the user asks a question to the system, we can again calculate the embedding vector of the user question and pull out the relevant pieces of this memory, which now correspond to Nass beliefs, essentially. And that's powerful because it allows you to check if the output which GPT generates based on the NAS beliefs actually has corresponding beliefs in NAS. If there's actual evidence which supports the answer, so to speak. So you get a bit of a, it's more transparent in a sense that you can actually check if the answer it gives you supported by the evidence. The way this is all achieved is there are multiple steps involved. For instance, I mentioned this piece of extracting relations from natural language sentences. And the way this is done, essentially GPT is prompted to take apart the sentence. So I presented different relational forms in negated and non-negated form, as you see here on the top. And each of them is essentially the triplet I mentioned. So you have some subject, some relation, and some object. So for instance, Garfield is a cat. So is a would be the verb. And just one example. So it's prompted like that. And then for question answering, GPT is prompted to actually refer to the memory items which are poured into the prompt. And we will see this in action. And here it's really important that the system is actually also prompted to provide memory indices, which has two purposes. We know if the answer is actually supported by NARS beliefs. Also, the concept priority is increased when this happens. So if the user asks questions about a certain topic, the reasoning system will actually reason more on this information. So you can essentially influence the system's reasoning by asking questions about a certain topic. We have directives in this combination in various reasoning tasks. And here, there's a quite popular one, which is Facebook AI Barbie data set, which consists of 20 reasoning tasks, which each contain 2,000 questions. It's different forms of reasoning which is required in the different BIBI subtasks, including deductive and non-deductive reasoning, various forms of spatial, temporal, and declarative reasoning. And the only shortcoming with this data set is that it's still focused on short-term question answering. So it's usually like five sentences and then a few questions. Then you follow everything away. Then again, five sentences, a few questions about it. So this data set is more about answering questions from a small set of sentences. It's not focusing at long-term question answering. But there is also the benchmark which Peter provided us, which is really focusing more on having a more long-term interactions with the agent, requiring it to have a long-term memory, essentially. get here on this data set so far is that we can already show that on various subtasks of Babi, like here inductive reasoning subtask QA16, we get 73% of the questions right. Well, with the GPT 3.5, it's 32%. In the meanwhile, we also got the numbers for GPT-4, which is 48%. So we still have an advantage here, still have a significant advantage with using our reasoning system for these tasks where reasoning is necessary, essentially. And it's also, when we look at the particular cases, like here is an instance of this QA16, I'm sure I'm not running out of time. Here you see, in this case, it's an induction task. So Greg is a frog. Brian is a lion. Bernhardt is a swan. Brian is gray. Greg is yellow. Lily is a frog. Bernhardt is yellow. Julius is a lion. in the screen and now the question is what color is urelius and you see the color of urelius is not explicitly mentioned it's also not deductively possible to obtain the answer you only see you only see that um yes so you essentially need abductive inference here to to make the to find the answer. And what's interesting is, so Euryus, we see Euryus is a lion. It's actually also not that easy for humans. And you also see Brion is also a lion. Brion is gray. Hence, there's only one example of a lion, and this lion here is gray. So according to inductive inference, that's evidence that Julius might also be gray if there's nothing else known than this basis of information. So here, this is particularly something which NASA is very good at. If you have a low amount of information and you want to make inferences according to the evidence. Here it's also here it's essentially an issue of contextual window limitation like with GPD 3.5 we only have like 4k tokens at most in the contextual window here you get 11% while with Nash GPD because we have this has long-term memory, it can give significantly better with 65% correctly answered questions here. Significantly better result. Here we also have the number for GPT-4 and immune balance, it was 28%. Patrick, just very quickly, did that include updating RGBT to use 4.5 as well, or just... Yes, it already supports to change the version spring to GPT-Core, so it already supports running the other model. The only thing which is not yet supported is running the open source models that there have been some new developments like Mekonos and some open source models which you can run potentially also on your Mac. And we are looking into this to make sure that we will be able to run it with fully open source language models, not relying on OpenMIR. 

S07 [00:24:54]  : Do you have an idea of how different they perform, the open source models? 

S04 [00:24:59]  : Yes, for Vicona, at least I can say that the large model of Vicona is at least comparable to GPT-3.5. And also, with Nash-GPT, I already tried the prompts. It's not yet integrated, but at least I tried the prompts to make sure that it actually makes sense to integrate. And it really gave me similar encoding results. I don't know, at least. What's also good is that there has been a lot of recent research in regard to belief correction. Let's say you want to customize your large language model for a specific use case. Say you're a restaurant owner, you have a Dutch panel where your guests can interact with you, and you want your AI system to give them information about the meals you serve to them. and you wanted to be factual and have information about the particular meals of your restaurant, usually you would have to do something like fine-tuning, or at least you would have to do a special prompt with your receipts. And an issue here is that GPD model will often hallucinate and give answers, seemingly confident answers, even though The answer is completely made up. Like here, I'm asking about a particular meal where it says, where GPT would initially say, I believe kefir bun and salad is a German salad made with green beans and potatoes. But it's actually not a German salad at all. It's made in Austria. So if you enter the corrected information here, it will create a belief accordingly. And the next time you ask questions about it, it will answer more factually without having to fiddling around with weights also. So it's a potential useful tool to have We will show a brief demo where you can see how this actually looks like. The memory is currently empty. The memory command, maybe I can also make the text a bit larger so this a bit larger. So currently, there's nothing in the memory of nulls, there's no atom. But if I, for instance, say that is on the chair, and it will now do what I described earlier, it will try to create relationships out of the sentence. And if we look into the memory of the system, you see here now, It encoded two relationships out of the sentence that the cat is on the chair with positive evidence. It also encoded that the chair is not on the cat, which is also valid. And so it can do this kind of relational encodings. And maybe there's also one thing which is relevant here if we look at also sometimes during the workshop, can we actually support persistent memory? And the answer is yes. It will also, when you run NullGPT, it will maintain a JSON file which you can actually it will actually generate a JSON file. The only thing is, in terms of printing it, it will not be that beautiful because it also contains embedding vectors, since the idea is really to have for each NAS belief, it will also have an embedding vector associated to it. So essentially, it's a hybrid form of memory where both the necessary representation is stored and also the vector embedding, as you'll see here, a lot of numbers which is associated with the beliefs. So you really have a hybrid database where you have the symbolic information, but you also have the vector information which can give you insights about similarities of the relationships. So if we, let's for instance, try some example where we have, let's say we're in a kitchen and there's some kind of table and let's say there are multiple things on the table. There's a cup and a plate on the table. It's already a bit more complex sentence, but as you will see, It essentially encodes the sentence into simpler relations, like an input sentence where there's a cup and plate on the table, and it's separately encoded to announce beliefs. Cup is on the table, and plate is on the table. So if we, again, run the memory command, we will see these representations. including the trough value and also the occurrence time at which it obtained this information. So you can now already do some more sophisticated question answering. For instance, I could ask, what are the things on the diagram? And essentially, now it answers, there's a cup and a plate and a table according to memory. And it also references certain values here. And we can now also actually look at what's this conclusion, so we can actually look at the prompt representation. And now it gets interesting, because as I mentioned before, each of these NASE statements also have a natural language representation, which is used to report into the prompt. We can actually query it. So this is the same information as there before, but in natural language representation. Essentially, the way it looks, the way NAS memory looks for the LLM. You can see it says copies on paper with this time, and it also says the confidence value. And you'll see at the beginning of the sentence, it also says the index. So it has actually really referred to this index. And they can really now look, how confident is it about this information? Or is this piece of information actually there? Is there really a cup on the table? So you get this kind of capabilities where you can really look into the reasoning systems memory. You can check if the answer was actually made up, or if there is really a belief for it. And of course, you can also do the NAS reasoning, like if it added information. 

S03 [00:32:46]  : How did you translate that? How does the NAS know what things mean, or what means and things means? 

S04 [00:32:57]  : That's a good question. It's originally in traditional reasoning systems that often there was semantics, model theoretic semantics in the sense that often you had the idea that the symbols in the machine refer to some object outside. And here in NASA, it's more experience grounded semantics, as we call it, that each of these terms They might not yet have sensory sensations attached to it, but they already are there. But they are not really referring to a particular object in the outside environment. It's something the system can acquire. How to say this? Essentially, you provide additional relationships about a certain term, and these relationships will determine the meaning of the involved terms. 

S03 [00:33:53]  : So if we rewrite the sentence, what are all the objects on the table, will it return the same answer? If I ask this question, what are all the objects rather than things? 

S04 [00:34:08]  : This depends on the background knowledge. Like if we say, Is that frozen? Oh no, it's still there. If we, for instance, say that on plates above containers, so we give it information that this actually categorized like that. Then of course, if you would ask other containers, on the table, then it would. API core failed. This can always happen. It will try again in 10 seconds. So the idea is essentially that you can give it information as you go, and it will remember it. And it also now answers this because it has because it sets these plates and cups above containers. And now we asked if there are containers on the table, so it has answered accordingly. So it has this capability now that you can more naturally interact with NERSC by simply using English as the language. But at the same time, it also supports NASIS, which is, if I still have, for instance, let's say in the robotic scenario, let's say there is some kind of, let's say it's in front of a chair. So from the perspective of the robot, there's a chair in front. So this is now just on a CS event, which we typically, for instance, in a current implementation, it uses the YOLO model so it can both the category and the location. So this would be an assessment which would come from the sensory encoding. But the thing is, even though this comes from the sensor, I can ask about it in natural language. So if I ask, is there a chair in front? So it will really reference this memory attempt And if we now look, it will say it has indeed this property of being in front. And if we look into the memory of the system, actually, we can also look at the prompt right away. What's the question? The prompt is dependent on the question. It's indeed mentioning the NERSC memory item which corresponds to it. So you can have NERSC input from various sensors and you can do English question answering. And this is also the next step because in the workshop, you have seen NERSC GPT, you have seen the robot which can already do various things. And if you include now NERSC GPT on the robot, you can essentially run the current solution Just in addition, you will be able to ask questions to the robot in English or give it course in English using same mechanism. It receives like here. And if you ask a question in English, it will be able to point to the beliefs of nurse to answer the question, even though the beliefs were formed from sensory model. And that's really a key. So it's really grounded representations. I think I'm running out of time. I think I actually have a question. 

S00 [00:37:56]  : Just a short question. Why does it say both that a container is a plate and a plate is a container? That's a good question. 

S04 [00:38:04]  : It says so with a very low confidence because if things are continuously used for the same purpose, like for instance, what is the notion of furniture? For instance, chairs and tables are furniture, right? And so also here, it will essentially form its own concepts, and it will form relationships between concepts. If they are in the same relationships with others, they can essentially acquire, how to say, they can become similar to each other. For instance, if I have two objects which I can both use to sit on, then the setting on relation itself will make it form a category which corresponds to things which can be set on. So it's essentially a form of inductive inference, which allows it to do this kind of relation of maybe one time it's a chair, maybe another thing it's a big rock. But for its purposes, it might both categorize it kind of a thing that can settle. 

S00 [00:39:10]  : So if we add something else and say that it is a container, will the confidence for container is a plate decrease? Because now there are more different objects that are containers, but container itself cannot be just like a plate then. 

S04 [00:39:28]  : Yes, there can be negative evidence. So for instance, if you look at, when is something a special case of something else? If you look at the trough value, we would no need to go into null one of the logic. How is positive and negative evidence of inheritance defined? And if you look at it, for instance, if you have a property which the general instance has, And the special instance also has, then maybe that's evidence that the special thing is that if A has a property and B has the same property, it's maybe evidence that A is a special case of B. But if, on the other hand, D has a property which A does not have, or maybe A has a property which B does not have, depending on the case, you will get negative evidence. I can show you later the diagram of the cases. We have a really good diagram for this. So it can receive also negative evidence, depending on which case it is. I think my talk is over. But the good thing is I actually finished with the talk. Thank you very much. 

S03 [00:40:56]  : Yes, this is open source. 

S04 [00:41:01]  : We have just uploaded the repository. If you go to github.com slash open now. You will see there's a repository. There's above our C implementation, but there's NashGPT, which you can check out from there. And when you run the build script, it will automatically download on and also compile it. So if you run the build script and run the in-store dependency script, then you will be good to go to execute it. 

S03 [00:41:30]  : That's in Python. 

S04 [00:41:34]  : Oh yes, that's pattern, the part which interacts with the LLM is pattern, and of course the reasoning system, which is in C. Well, you will need a key, an API key. 

S03 [00:41:47]  : Okay. And how do you get that? 

S05 [00:41:51]  : You just, you have to pay for it. 

S08 [00:41:54]  : All right. Thank you very much, Patrick. That was exciting, very exciting. So next, we'll go to Anton. Welcome, Anton. Thank you for leading this excellent session. Self-tuning hyperparameters for unsupervised cross-lingual tokenization and morphoparsing, which is easy to say and even easier for INLP to interpret. Thank you very much, Anton. 

S06 [00:42:26]  : Hello everyone, I will take a little bit of time in the beginning of my talk to present the groundwork that has been presented in previous NLP workshops and then move on to the recent results that are advertised on the agenda. So before we start warming up, I suggest everyone to solve the puzzle. So here you see three lines of plots and each line of plots correspond to one of three languages. So the languages are English, Chinese and Russian. So please make your guess and decide which language is English or Chinese or Russian. and you will have the answer in the end. Okay, now we go. So the plan. First, quick introduction to the concept of the interpretable natural language processing, then overview of some applications that we have developed over the last years, and then move on to some fundamental studies. including the segmentation for text, words, and morphology elements. Introduction. We are working on the subject of marrying the distributed language modeling and formal symbolic language modeling. We anticipate that there are two worlds and these two worlds are corresponding to two different modalities of thinking accordingly to Daniel Hahnemann, like system number one and system number two, and we can learn language in any of the two models, and we potentially can transfer the knowledge, linguistic knowledge and semantic knowledge from either model to another model. So that's the belief. And the way the model can be represented is potentially is any formal way of representing the knowledge like Lean Grammar, which we'll describe further in a minute, or you can consider any graph-based language model representation like we are developing in the course of a Yajin's project. For example, here you see how the sequence of tokens representing specific sentence expressing certain semantic relationships between some medical treatment and some medical effort effect and the way the medical treatment is connected with medical symptom and you can represent this connection by means of the graph where the nodes of the graph are corresponding either to particular tokens or n-grams connecting these tokens into the phrases or disjunctive associations of the tokens connecting the synonyms or words with similar meaning and then further up in on the graph you see some more generic semantic concepts or grammatical concepts like nouns or verbs or you also can have some what we call templates where for example the subject predicate object phrase with abstract subjects predicates and objects is a simplistic transitional sentence template, but of course you can have more complex templates and you have more specific templates with very specific subjects and objects. Applications. One of the applications is what we call AIGSD patterns text mining, where we consider the same graph-based language model, maybe use it to solve three different tasks. The task number one is categorization, so we believe that we can use certain graph-based patterns to assign the category for a particular piece of text, like here we can see that given this text, a particular pattern is found in the text, maybe this, and then based on these patterns, this text may be attributed to category healthcare, and then another example is that based on more specific pattern you can extract particular segment of text corresponding to particular semantic relationship or clinical case or relationship connecting multiple entities and finally within this piece of text on the right you can see what we call property attribution or entity extraction where you take this piece of text corresponding to a semantic relationship and you get arguments of the semantic relationships as particular attributes or meanings of particular entities corresponding to different attributes of this text. And the way we do it is that we describe a certain way of expressing these complex deep patterns, where each pattern can be built as a hierarchical construction of different sets, where it can be disjunctive sets, conjunctive sets, and what we call M-skip and Gram, which are conjunctive sets with order implied between the elements of this conjunctive set. And then every element of this set can be either token or regular expression or any variable that can be filled or it can be another set so multiple sets can be heterarchically included in one into other and here is the example how you can handle this so you Once you have this pattern, that's the first part of this expression. You can convert this pattern, apply this pattern to any piece of text. like second argument of this expression, and then after you apply this pattern to this text, you just get the semantic object, you get the relationship with attributes of this object field. Another example of our work is that using these patterns expressing particular semantic or psychological attributions of a piece of text, we can do semantic analysis and we can do sentiment analysis and we can analyze cognitive distortions like in our social media And then the analytical framework, we can detect positive and negative sentiments in the same text and every text can be attributed with different blends of positive and negative sentiments to make fine grained analysis of the sentiments and social media text. And the way we use this framework makes it perform traditional neural network-based models. For example, here you can see different out-of-the-box sentiment analysis models taken from the open source, about 20 models. And the second from the right, from the left, so it is AIigence Barker response to the level of sentiment accuracy out of the box based on our model. And it has the same performance as Finebird, fine-tuned for financial domain. And the text that we were using in this study at Singularity DAO were also attributed to financial domain and but the problem that we had with neural network birth-based sentiment analysis model is that in order to improve this model we were in the need for huge corpus of text to train but with interpretable agents model all what we had to do we had just to fine-tune these deep patterns based on n-grams and then after fine-tuning these patterns based on linguistic analysis we were able to improve the accuracy up to 0.6 percent. And here another example of how we were able to use this fine grained analysis of emotional expressions and text. We were using the patterns designed to detect cognitive distortions used in cognitive behavioral therapy employed by cognitive behavioral therapists. So for example, here you can see that particular cognitive distortions may be preceding changes in the price. And even better, they can be used to predict changes in volume of trades on the financial markets. and we were making this analysis of sentiment and cognitive distortions to actually predict the price and we were able to find that using the attempt for price prediction makes it trading more efficient make more efficient trading possible for example on the left you see the results of price prediction based on the social media and cognitive distortions based price prediction i am so based on this price prediction you can get more much more profits these green bars and if you don't use these predictions and just trade based on the percepted price change you are getting much worse returns so most of the trading strategies give you red bars so you have profits and only one in one case you have small profits so on the left you can see that our predictions make trading more profitable And other examples that I will talk about, I present in some applications that can be done on based on this graph based language modeling about language generation, for instance, for example, if you have such a language model for particular process of giving a hand, like you have the noun categories, you have verb categories, you have body part, you have process, like process of hand given, and you have different languages like English and Russian, so you can deal with multilingual models where different vocabularies are associated with the same semantic concepts across different languages. and then you have notions of the mood like indicative mood imperative mood and particular patterns like sequences or conjunctions of words expressing these patterns then in having such a model you can put some attention and here you the attention in terms of latest bird model comes into play because when you have the graph a model of the graph model of the language and you when you put some attention on particular nodes like if you want to then you can actually generate the sentence from the graph for example if you want to have a hand-given process expressed in Russian, then you put some attention on hand-given, and then you put some attention on Russian, and then, for example, if you want to force a certain person to give you a hand, you put some extra attention to the imperative mood, and then spreading this attention over the graph, it will generate your Russian sentence for forcing someone to give you a hand in imperative mode. Another study that we have done is based on the Lingrammer. So the Lingrammer-based language modeling is explored a lot, starting the earlier works by Ben Gertzel and Linus Webstance, and a few other studies that I will present in a few minutes. And what we have done beyond the conventional parsing, which is typically done with linparser, we extended the use of the lingrammar dictionary to do the text segmentation and text generation. For example, if we want to do The text segmentation, we just go through the sequence of tokens generated, for instance, by speech recognition engine, and then we just parse these texts till the point where we have the complete parse, and we stop parsing when the sentence terminates, and then we start another parsing. uh and if you want to do the third generation again we can have the bunch of words which are pretending to generate a sentence like about the cat and the table in the talk of the previous presenter And then we can use the grammar to generate the valid sentence based on this bunch of words. For example, here is the example of how we of the algorithm that we can use to generate the sentence based sequence of words. And here is the accuracy of our results. So based on some simple corpora so far, we were able to get quite good accuracy. and also we were able to extend our approaches for question answering so we were able to design the system where asking the question to a system and making the words being a part of the response to the question can further be used to use our language a sentence generation engine to generate the valid answers. Like for example, given mouse and cat, we can generate the sentences either about cat caught a mouse or the mouse caught a cat, depending on the semantic context. And also we have tried this question-answering framework on some basic corpora, comparing to four different techniques like Bert, Elektra, Distilbert, and Roberta, and we were able that using most of the metrics we were getting superior results over the competing approaches. Now, getting to fundamental studies. The study that we were doing at Singularity.net project for a couple of years was in supervised lean grammar learning, where we were able to get some initial steps for automatical creation of the lean grammar dictionaries out of the unannotated corpora. And the way we were doing this was that we were taking the pre-technized sentences, and that gets us closer to the main subject of my today talk, So we were taking the input corpus and we were tokenizing it and splitting it into the sentences by some hard-coded tokenization and punctuation handling rules. But once we were getting the sentences, like I saw the saw, I saw the truth, I saw the see, we were able to create different sorts of the parses. And then using different sorts of the parses, we were able to generate the grammars in the lin grammar formalism. So for example, here you can see that using the same text. the same corpus of text can be used by either in grammar to pass the text to generate the pastries using the English grammar dictionary known in advance, or we can use statistical MST parser based on the mutual information to generate the parse trees on the right and then these parse trees were used to generate what is called the disjuncts in grammar and then using these juncts we can find that words belonging to particular grammatical categories can be actually cluster it in the multidimensional space like it is done with word embeddings technique but it is done more accurately And also we can build hierarchical clusters describing the hierarchies of these words, like finding particular categories of nouns, of verbs, of words describing the temporal happenings, particular days of week, times of day, and so forth. Unfortunately, what we have found during this study is that the quality of the grammar that we are able to learn is semi-linearly correlated with the quality of the parses. And we were not able to find the good quality of the parses using the MST parsing approach. and also we were using to get the parses using the BERT based contextual information taken out of the corpus trained on the BERT and then using every sentence run through the BERT to find contextual dependencies between the words in each individual sentence this way or another the parses that we can get for input for grammar learning were not good so we found that in order to get the good grammar learn from the parses we need to get the parses and so far we were not able to find the good parses and here is the starting point of the study that i will present in the remaining 20 minutes so uh the problem number one that we have decided that we need to solve from the bottom up is tokenization because in the earlier study that i have been presenting we were not we were just taking sentence segmentation and word segmentation word-based segmentation implemented by hard-coded parser so we decided to start this from the level zero from the ground zero and we generally consider three different problems that we are going to solve using the unsupervised language learning bottom-up first problem is segmentation okay so we need to segment the text into sentences then we need to segment sentences into words, then we need to split out the punctuation marks, then we need further to split words into the morphological units, because morphological units are important to identify the relationships between the words. On the other hand, when we, and finally, if you are dealing with written text, you obviously need to isolate particular letters out of the context of the work, right? So once you are able to separate individual letters or words, you need to do the clustering. So you need to separate what are the punctuation marks, what are the vowels, what are consonants, what are different ways to write down the same letter, and so forth. And you need to do this iteratively, because after you're done with segmentation at one level, you need to do the clustering on the other level. or on the same level and then you need to go back and do segmentation on the other level and get the clustering on that level and at the same time you need to do the parsing so you need to find the meaningful connection between the pieces of the words between the roots and endings of the words you need to make the further relationships between the subjects and verbs between the objects and verbs, which gives you to the identification of the parse structure of a sentence or a word. And we can find that parsing at the word level is, I mean, parsing from morphological perspective and parsing from the grammatical perspective at the sentence level is somewhat similar. and this this the slide that you are seeing here is important and we'll have back to the something uh underneath of this slide uh at the end of this talk when we will be discussing results of our study so here you see on the top you see two different uh sentences expressing the same same meaning right it is impossible in english it is and it is not possible in english right and the meaning is the same the difference is that in the first case there is single world single word impossible and in the second case there are two words not possible but uh in in both cases you have him and possible uh like uh associated together because him serves like a not related to possible right because him negates the possible right so there is a connection between the prefix and the root of the word and if you find chinese you can find that in chinese the meaning of the single would call call it writing on the left is the same as separated writing on the right so if you take I am sorry I'm not familiar with Chinese myself but if you put every text in good inch in Google Translate the meaning is the same. Further, if you put English text from the top and put it into Google Translate, you'll get the Chinese text on the left. But if you take Chinese tokenizer called jiba you and you try to take a nice this text it will identify the stride separate that characters as you can see on the right but the meaning connects in this characters and the same and the meaning is the same as for english sentence and the grammatical structure and semantical structure of the sentence sentence is the same So moreover, let's move on to how we were approaching the problem of segmentation and cluster for in respect to the words, punctuation and morphological units of a text. We were considering two different metrics computed over the graph based model created from the corpora. So what we were doing, we were computing all transitions across the training corpora, so we were going over the training corpora from the first letter to the very last letter, and we were computing the transition graphs over n-grams with different n, like one character to character, two characters to character, three characters to character, and we were counting two different metrics on each transition. The metric number one was what is called conditional probability, which is well known for anyone who is doing this neural network-based language modeling. And the second metric was transition freedom metric or freedom of transition, like it is called in the paper that we, where we found this. And that slide here indicates the difference. For example, if you take a word how, and compute different frequencies of words that can appear after how on google the numbers here are taken from the google search engine index you can just validate these numbers entering the different searches and see a number of results that you can get So here you can see that there are different frequencies of two, do, many, long, are, and so forth after the how. And based on these numbers, you can get the conditional probability of give every token appearing after how. And further, if you have how many, then after how many you again you have the people you have the times you have countries and states, how many people how many times how many countries, and you also have the frequencies and you can computer compute conditional probabilities of people and times after how many. But these are conditional probabilities. But besides conditional probabilities, there is what is called transitional freedom. For example, after how you can, you can have only five transitions. So transitional freedom of how is five. But after how are you have only one transition, only you can appear after how, how are so transitional freedom of how are on to the right is one, but transitional freedom of how many to the right is for. so we can have both transitional freedoms and conditional probabilities and then we can also consider mutual information of pairwise tokens occurring on the graphs and we do and we can do further analysis for considering the changes in this metric across the Text when traversing the text when processing it and make decisions on whether we need to put the break in a sentence or within the word based on these Changes on this metric and by the way here you can see the illustration of how their transitional freedom works and accidentally or surprisingly this notion of transitional freedom makes somehow resembles the notion of free energy suggested by for carl freeston as a key principle of evolution of thinking matter like brains are designed to minimize uncertainty into in other words and here we can see that if you are entering how are then answer uncertainty is minimal. So after how are only you can appear with a small exception, but if you enter how many the uncertainty is great because after how many, anything can happen. So after how are entering, how our transition freedom is minimal and how enter and how many gives you a large transitional freedom. And the idea that we were exploring that efficient knowledge representation models are minimizing the transition freedom and minimizing the transition freedom within the model they are minimizing uncertainty. So here, here is the way how we were trying to do that. text segmentation and tokenization without of any supervision based on different metrics that I have presented. What we were doing, taking any sentence like Tom said, quote, Jim's right and quote to end. okay so you see that punctuation and you see different kinds of quotes and what we can do we can compute different metrics going through the text from the left to the right and back from the right to the left and we are computing these values based on these metrics, either conditional probabilities or transition freedoms on both directions. So when we are going left to right is blue, when we are going right to left, that's orange. And we can see if, for example, if we are using the conditional probabilities, you can easily you can isolate the words like every space is identified by maximum conditional probabilities, but you still have false negatives, because you cannot identify the quotation marks and also you get false positives. So some words like Tom are split apart and that's not good. And by the way, uh, when doing so for every metric, we were also able to use different N so we can, we were able to compute this metrics using different orders of N grams, like unigram, bigram, two gram and so forth and so forth. but where so so that's what we get with conditional probabilities so it's not work we have false positive false negatives but when we were using that transitional freedom we surprisingly found that we can perfectly identify all punctuations include all quotation marks with high accuracy with n equals 1 so at the top you see that we have neither false positive nor false negative when we increase the order of n grams we still have some false positive and false negative but n equals 1 for unigram is seems perfect And here is the heat map of the search for multiple parameters, so we were exploring different hyperparameters driving this tokenization, for example, on the vertical axis there are different combinations of n, like one is n-gram, two is b-gram, one two is combination of unigram and bigrams and so forth, and then on the from the left to the right on horizontal axis you see different thresholds where we cut off the value of the metric in order to detect the world break And also we were able to apply with different thresholds for pruning the model, like no pruning corresponds to filter 0, 0.001 corresponds to particular threshold for pruning the low frequency. low frequency transitions and here we can see that for English with unigrams with threshold 0.4 and pruning filter 0.001 we get 0.99 0.999 F measure for the organization of the English text with lots of punctuation and lots of different quotation marks. Here are the results. So what we have got, we have got that for English, we can go out F measure 0.99 or 91. That's the first number. The second row number is the F score for organization based on the lexicon. So if we were considering lexicon to be my ground truth for the words in the lexicon and if we're using lexicon to take words out of the text word by word in some greedy beam search algorithm then our tokenization is the same as for english if you consider russian then we can get 1.0 doesn't a score for tokenization using our algorithm based on transition freedom. And it is better than greedy beam search based on Russian lexicon. For Chinese, we find that it's not as good. So we find that Chinese green beam, greedy beam, lexicon based search gets you 0.83 accuracy and Chinese freedom based tokenization gets 0.71, not that high, but On the right column, you see that we have tried to do something else to validate our algorithm. So that's precision of the lexicon discovery. So what we have done after the tokenization of the corpus, we were evaluating the corpus that we have learned from the corpus against the known lexicon for particular language. And we have found that both for English and for Russian accurate precision of the lexicon discovery is the same as tokenization. F score. But for Chinese, the lexicon discovery precision is very high. It's pretty close. So it's it's above zero point ninety one ninety ninety two. And it's much higher than the accuracy. So what we suggested is that even though we were not guessing the tokenization for Chinese, the words that we were finding in Chinese text were still valid Chinese words. So getting back to the slide on Chinese tokenization ambiguity that I presented 10 minutes ago, We can speculate that maybe tokenization in English, sorry, in Chinese is not as impossible as in Russian and Chinese. And so our mistakes is forgivable. Another example of what could be done based on that graph-based model of the text is clusterization or categorization, unsupervised categorization of the text. So we took Russian literary corpus for the detection of the age, Rus' age corpus, and we surprisingly found that beyond been able to identify Russian vowels and Russian consonants and all sorts of punctuation, we were able to identify all digits and English vowels and English consonants. But the study that I have been presented in till now was based on the assumption that we have some ground truth test corpus where which we can use as a reference to find the set of hyper parameters in order to set up our tokenization model. Right. So after the training, done, we need to find the best parameters that we can set up in order to perform tokenization based on this model, like number of N for N-grams, threshold, threshold for the detection of the word break, threshold for the pruning of the model. And we don't know that in advance. So what we have tried to do, we have tried to find how we can find these hyperparameters without any cultural specific knowledge about any prior knowledge about the tokenization rules, about any reference corpus, about any reference tokenizations. So we were speculating that a particular matrix like compression factor or anti-enter normalized anti-entropy may be used as a clue to learn these hyperparameters. So here you can see three plots with different hyperparameters. On the top, you see the F score. for brown corpus tokenization. And the second plot is what we call compression factor. Compression factor means that we can consider the volume of the corpus as subject of compression. And we assume that if we tokenize the corpus, and replace the tokens with indexes of these tokens within the text, we do the compression. And then the compression factor is the extent to which The compressed with tokenization corpus size relates to the size of the original corpus. And here you can see that whenever we increase the compression factor with the same sort of hyperparameters is also increasing the F-score of tokenization. And the bottom plot corresponds to what we call anti-entropy. So we can compute an anti-entropy of the original corpus. And then after replacement of every token in the corpus to respective world, we can evaluate the anti-entropy of the sequence of the tokens within the tokenizer text. And then we just compute the anti-entropies. And then we can see that, again, anti-entropy is increasing with the same hyperparameters, which are increasing the F-score. And here is the answer to our quest. Here you see the results where we assess different corpora. F score on the vertical axis will always uh see that mechanization f score and on the bottom axis you see here you can see the anti-entropy see here you see compression factor here is something else that we called cross split f score which is another metric that we use it to compute that we were trying to use as a clue for finding culture agnostic objective target for optimal tokenization And then there is some blending composite of the three metrics, but you can see that for English and Russian, it is mostly linearly, semi-linearly correlated between, the F-score is semi-linearly correlated with both anti-entropy and compression factor. For Chinese, it appears not that clear. Moreover, for brown corpus on Chinese, you can see that the relationship is quite not that obvious. And also, you can see that on the other hand, the correlations are the same on the left, on the right for quite different corpora. Because on the left, we were using different corpora for training. And for testing. And on the right, we were using the same brown corpus used for training for English, Russian, and Chinese. And we were using the same magic data corpus used for testing. And regardless of corpus that we take, different corpora or the same corpora, we have the same dependencies across different languages, which can suggest that we are not hallucinating about these connections between the tokenization F score and culture agnostic metrics like compression factor and anti-entropy Here are the results that we have tried to dig deeper into Chinese So we have found is that if we are getting smaller Chinese test corpora like 100 set lines of Chinese text and 1000 lines of Chinese text the connections between uh the uh anti-entropy and compression factor and f score are not that obvious but while once you are getting to 10 000 sentences in chinese uh for not sentences actually lines so that the test corpus in chinese Then with these 10,000 lines, it gets much clearer. So you see that both anti-entropy and compression factor on corpus on the left and the corpus on the right gets into semi-linear dependencies between the culture-agnostic metric and the F-score. So we also have tried to validate these results based on different Chinese tokenizers. On the left you see ground truth tokenization from SIGCHAN corpus with manual tokenization and on the right you can see that the ground truth, the results based on the ground truth evaluated by Jeeba, a programmatic tokenizer. And finally, Finally, what we have tried, we have tried to explore how far we can get with the morphological analysis of the words in English. so here you can see that uh if you take some reference english technization sorry morphological parsing of the words as a ground truth on the left we can see to use different techniques to create some artificial morphological parses like morphology based is based on the greedy beam search based on the dictionary of English morphological units, then, and it's somewhat around 0.5 F score. Then if you get BPE, byte pair encoding, traditionally use it in machine learning and deep language modeling, you get pretty low. F score, if you get novel dynamic pair encoding approach, you get 0.55, which is the champion and using our transition freedom based approach, the best that we were able to get is 0.25 so far. And here we also have tried to see if there is some connection between our ability to get high morphoparsing accuracy and ability to identify morphological units correctly. And we are finding that there is some semi-linear correlation between our ability to find right morphological units and our ability to provide right morphological parsing. However, the correlation is not, I would say, quite reliable detected as in the case of tokenization. And what is even more interesting is that if we try to find these hyperparameters for morphological parsing based on the same metric that we were using for tokenization, like anti-entropy on the left and the compression factor next to the right, we were surprisingly finding that for morphological parsing, the compression factor is driving morphological, compression factor is driving morphological parsing accuracy. So you see my linear, linear connection between the two. But for anti-entropy, it's quite opposite. and here are results that we have got for an awards with more than 10 letters in a word and if we are sorry with uh yeah with more than 10 letters and uh here here was the difference ah these are results at the top you see what happens if we have more than 10 letters in the word and at the bottom you see what happens if we have use any words and these are results with n equals one and the best of score is 0.42 and here's what is happening with n equals two so based on n grams We get a little bit better F score based on big RAMs, but again, the compression factor provides linear connection between a compression factor and the morphological parsing F score. But the anti-entropy provides opposite connection, which is surprising and needs more thinking. And here you can see that on the left, you see the connection between these culture-agnostic metrics and the score. And both are linear and on the morphological parsing, these are opposite. And like in the mirror, which kind of puzzling so far. So, takeaways from our studies that grammar and syntactic semantic word categories can be learned, given we can learn tokenization and parses unsupervisedly. However, parses cannot be learned well given bulk non-carusulum. And for supervised learning using mutual information, BERT and MST parsing, tokenization and character categories can be learned unsupervisedly based on the transition freedom matrix as we have found last year and this year, then hyperparameters for unsupervised text segmentation learning can be found based on these culture-agnostic metrics such as compression factor and anti-entropy in case of tokenization, but unsupervised learning for morphological units and morphoparsing may be possible but remains untrivial and needs more study. So here are the answers for the puzzle at the beginning of the talk. And I think everyone got it right for Russian, for English, and Chinese. And thank you. 

S08 [01:28:22]  : It was great. Thank you very, very much, Anton. That was fascinating. Any questions from the room? information overload. Time to go parse that, right? Any questions from online? Okay, great, fantastic. Thank you so much, Anton. That was really spectacular. Not only hosting this session, but giving us all of that as well. We'll break for 10 minutes, 15 minutes, 15 minute break, and we'll meet back here at 10 after. I think there's still coffee over there, coffee machine here, and we'll get into our last three presenters. Thank you. 

S05 [01:29:22]  : Thank you. 

S08 [01:45:18]  : All right, welcome back to the second half of our session, last session. We're going to start off with Zakhar Ponimash and Viktor Nosko, hopefully I got that fairly correct, with ExplainItAll Library, explainable and interpretable AI for generative transformer neural networks. So thank you very much. 

S02 [01:45:55]  : Hi, I would like to present today an experimental library. It's a library for interpretable AI for generative neural network models, transformers. You know all about transformers. First of all, what is the task for explainable AI? All you know that we all suffer from transformers, from GPT-3, from charge GPT, from even GPT-4, but we all know that text generation must become controlled. We want to control, we want to take out hallucinations. So, objective. We must explain why the generation of some transformers make some kind of generation and what it depends on. We must evaluate the applicability of the model to some downstream problem. And we won a grant from the foundation to develop some kind of library, especially for transformer models. There are a lot of libraries for other types of AI algorithms, but transformers usually lack explainable and human readable explanations what they do. So we have a challenge. Current libraries visualize attention by words. It's not human understandable. Generation is dangerous. No factual control, unnecessary topics, entities may appear as some kind of not relevant to the whole topic of a generated text. We cannot trust any kind of question answering system in complex and sensitive industries. You know, it's medicine, it's law, and other domains. And also, we don't have metrics to to understand what is the quality of interpretability of AI models. And so we started research. What is the goals of our research? We must We proposed explainability methodology, so we need to offer a methodology for the interpretability of large transformer models of any kind of a model. including GPT-like models. We need a methodology for evaluating the quality of interpretability, depending on model parameters and architecture. And we need to offer a new interpretability technique for question answering systems. with explainable parameters and and metric of completeness so on the right part of this slide what you see here you see on the on the top green a part question and an answer a typical answer from the model so question is what drug let us it's some slots you can fill the slots what drug something in liquid form is acceptable to use for children with disease, some disease, with a given concentration, and so on, and so on. And you, as an end user of the system, you can pass this question to the generative model, and you want some kind of explainable answer. And in a long form, this is called often LFQA, long form question answering. So you get some answer, children with age from five to seven years. can use these drugs and so on. You can read this answer here. So what do you want when you read this answer? You see here that you don't know really. Can you trust this answer? maybe not, maybe it's a hallucination in some kind of nurse. So, ExplainedTool, our library, should offer the following parameters and should generate or present ExplainedQ asset quality Quality metric. The confidence level of the answer is 80% and this 80% based on the sources S. So you should hear that explanation offer some kind of confidence and what is based on. Second, interpretation. Interpretation is a human readable interpretation. So you can read here, entities L and X occur with frequency F in sources. So this is description tokens, temperature, and these parameters that you read here is parameters that this interpretation depends on. Usually, these parameters is isNamedEntityRecognitionResults. So what is the common approaches to the explainability? Here we emphasize very briefly some parts of the approaches to this problem. Here is a casual self-talk from DeepMind, an article where you can, in this article, you can read some approach when a model talk to this model but upgrade internal parameters when it's talk. In some kinds you can use this approach, in some kinds it's not appropriate but you know that now GPT-4 is used for evaluation of the other models. For example, you can, you can evaluate llama, vikuna, and other open source models with GPT-4 explanation, and this is, and, and this approach, mostly good correlate with human explanation. It's a very cool finding nowadays. Yes, early this approach was bad, but when we now have GPT-4, you can use this approach. And here, some kinds of requirements for explainability. I will briefly repeat. It's grounded. Model should be grounded on the sum data set. it should be flexible, it should be minimally inferent. The use of any explainability model should have little effect on the underlying generation model. It should be scalable. That means that approach should work on any parameter, on any number of parameters for the model. It should be faithful or truthful. So you have a common problem of the egg and chicken. when you got an explanation for the model, you can ask further, but how can I trust this explanation? Maybe this explanation can lie in me too? So it's a chicken and egg philosophical question. So we here in our library, tackle this problem. teaching structure. Here we show that maybe you know about reinforcement learning from human feedback but here we show another approach and there are a lot of another approaches how you can train transformers. Here another schema of the training, restructured pre-training. And this schema is useful for explainability because you can, after training, you can build a grounded explanation system and this system will be grounded on the parts of the data set that is structured. So you add some parts of the labeled data set. from the text. So maybe you can train DIRT or GPT-3 or another model and you can add, for example, summarization of this article. When you add summarization to the training of GPT model, you also can get a better result with a smaller number of parameters. But if you have these granular parts in your dataset, you can build a grounded explanation system. Here is another library, Sharp for Transformers. As you can see here, you can see visualization of the screenshot, but it's a bad, not good visualization. You can see that attribution show you what parts of the text belong or attribute to the positive or negative parts of text. So, but that's all. That's all what this library can do. Another approach is CAPTUM and we edit, we rewrite CAPTUM ablation approach in our library. What is ablation? runs the sequence through SBIRT and you replace each token with a PAD token and then counts element difference between the original vector and the one obtained after replacing. And so the gravity is the contribution of the word to the initial vector and the gravity will be difference between between those two vectors. So this approach ablation show you how important some token is for this sequence. And what we have done, we have done interpretation for images, what you can see here, you can pass sentences, and then you get a set of sentences produces a list of important words. So on this screenshot, the bottom part is important ones for these sentences. And it's a good feature here is that you can now test any model from SBIRT. SBIRT is a sentence transformers library. Here, interpretation of the model schema. we give text input with the assumption that a model is sensitive to some semantic axis or cluster. We form cluster using word vectorization algorithm or Word2Vec or others. Then for each cluster we count as the total importance of words it is for neural network. And then we calculate CAPTOM plus average Euclidean distance. We count this for a wall sample and how much the word we erase, how much this word changes the vector on average. This indicates how entire cluster have influence on the particular meaning. So here is a schema of the interpretability. What we've done further, we changed some phase, we changed some of vectorization. algorithm, we changed it to Faiz because it was fast and provided better evidence. And the final step here, test of the interpretation. What we do here? You can see on the screenshot that we name a cluster. Above cluster is animals, dog, cat, fish, and so on. It's a very simple cluster for the demonstration purposes. And you have a second cluster of drugs and some kind of drugs here. Then you want to test some model. In this case, it's Robert. It's a BERT model. And what you get in result, get a text generated with some kind of rules that this model, this transformer. is sensitive to the animals cluster and also you can ask but maybe for the drugs it's also sensitive but no this model is not sensitive for the cluster of drugs, what does it mean? It means that you cannot use this particular transformer model for the drug discovery, drug question answering system. You maybe should train this model. So it's a part where you test a model, should you use it or maybe you should not. and the next part, generative neural networks. The interpretation of generative models work as follows. Using the library nseq, we use nseq library, We determine the importance of each token in the sequence to generate the next one. Then we aggregate the tokens into awards and determine the importance of each award to generate the next one. Next, we extract entities, also we call them semantic clusters, and calculate how much the model looked looked at the clusters when generating a new cluster. This approach allows us to aggregate information at a new semantic level and deduce the rules by which certain entities are generated. And at the last stage, we can, with some expertise, set the system with ideal entity generation rules, and then use apparatus of fuzzy logic to analyze how much we trust this generation. For example, to get a numerical value and then use it. For example, to run the versions of the answer using the LLM hallucinations. So, here we are trying to interpret some model, we also tackle a problem of hallucination of large language models, but not always large, medium, any kind of language models. One of the challenges is to unify token importance. We can use embedding distances, gradient methods, attention learning methods, and so on. But all of them need to be able to be reduced to the same scale. At such a scale, we choose the probabilities that a given relationship is non-random. we obtain a real distribution of importance scores, then approximate it with a mixture of distributions using the EM algorithm and then integrate this mixture to obtain an integral score. We choose three-component Gaussian mixture, and we also have experiments with five components, but three-component is the best solution. In simple words, what you see here. earlier on the slide we show that usually you got 80% for example 80% of stability of reliability of an answer but you can ask but what is the threshold 80 percent is the best number or we need to understand that we should wait for 90 percent or maybe starting from 10% it's a good result but below 10% it's a bad result. So this kind of questions about the threshold but it's wider. This question about kind of curve. It's a normal distribution or Gaussian distribution. What is the form of this curve in your data? So it's a very important question because an answer is is used by the end user. It must understand 70-80% of reliability is good or bad. This slide shows the plots with the connections on the left being returned using the integrated gradient algorithm and on the right being recalculated by our method inter-probabilities that the connection is not random. And here you can see visualization of this heatmap and understanding how you look on these tables. In addition, we have corrected the work of the library nseq. We used nseq as a base library and a library is another independent part of nseq, so we must make some corrections to the insect to make it understand Russian language and it's now optimized the speed of its installation. On the left, how the library worked with the Russian language before and right after. Modification, it's a very fast installation, and you can access this new library on GitHub. Here's an example of use interpretation. What, what you see here on the screenshot, you choose some model, some transformer, a small model from SBIR. Then you as an end user define a cluster. You define a top k. Top k is a number of words that define this cluster. And you define also a centroid of this cluster. So centroid is a very important word in some clusters. And then you call a question. How can you make a better health for a dog if a dog has some kind of disease? And here you can see an answer to to avoid disease with some kind of drug and you can add table where you can see should you trust this answer or not. Also, we have developed interface for the library, it's a user interface when you can upload a model or define a cluster, and it works with a popular Gradle library. Also, here's demonstration how it works in here we demonstrate a problem some kind of challenge when you use question answering systems you can ask some system but always you have a problem with your question. The question is not full and usually it does not consist of useful information that is used in the model to make a correct answer. So the next big thing in progress, how can you make a question more correct? And here, and you know that pharmacy, manufacturers, science also need some kind of this library. Here are some perspectives of use of this library, maybe you know, robotics, systems, chatbots, and so on. uh or here here team uh we we develop uh this this library library uh experimental library will be open source uh will be free uh my license pre-licensed for commercial use. So you can download it very soon and use in your projects. Thank you. 

S08 [02:13:34]  : Thank you very much. Excellent, excellently presented. Any questions from the room that we can take? Any questions from online, Peter? No, great. Excellent. Well, look forward to it being free and available for testing soon. And you're absolutely right. The criticality of getting these systems trustable for these critical industries can't be overstated. And we're all getting so much closer to that. And I can't wait to see how we break through in many different approaches. All right. Fantastic. Thank you very much. Next, we'll have Andres Suarez and Man Hin come up. They're going to be talking about interfacing linguistics and logic, unleashing the potential for large language models through augmented AMR and higher order logic. Welcome. Thank you. 

S09 [02:14:32]  : Thank you. Thank you. Yeah, so let's see. Thank you very much. Actually, what we are going to share today is about using large language models to map natural language to logic representation, and then we use it in a logic engine to improve the reasoning process, which Coincidentally, it's quite similar to what Patrick has shared earlier this afternoon in his presentation. It's also interesting to see how many different peoples are pursuing this way. It shows how important this work can be and how impactful it can be. we are still in the early stage of our work. And we have also actually started discussing with Patrick and see how we can collaborate and align our efforts so that we can get better results. So yeah, so because of that, I think maybe we will try to focus our presentation a little bit to focus more on the differences that different kinds of work that we have done and also the future directions of the project. So let's start with explaining why we are doing this project. So one of the many reasons that we want to do the work is because we know that lots of knowledge in the world is anchored in natural tests. Basically, they are unstructured tests written in natural languages. We think that doing logical reasoning over those unstructured natural language tests is a crucial step for, for example, automated knowledge discovery, which, if we can do that, can benefit many of the scientific domains easily. Apart from that, actually, we can even improve the inference process by using the LLMs to suggest inference steps. And we can use that as a guide for the inference control in a logic engine like PLN, so that we can reduce the search space dramatically. I think we can get back to this later. uh as a future steps and well I guess needless to say drawing like correct conclusion based on explicit uh knowledge has always kind of always been a goal in the field of AI and AGI so and that is very important however if you look at the um most fashionable AI systems nowadays like the LLMs we notice that they are not still very not good at doing all these logical reasoning so I think we everybody in the room has already seen many examples in the past few days so maybe I would not go through this much detail so Basically, we know that the language model like GPT-4 sometimes may draw ambiguous reasoning and even can lead to a wrong conclusion. Like in this case, it would think the bear is red. Still, we think that the current LLMs, they are still very capable at doing what they're doing, like language comprehension and generations. And so on the other hand, the logical reasoning is kind of very reliable in a symbolic setting. So we are trying to bridge the two, get the benefits of the two systems. Yeah, so that we can make the process more reliable. So in here, we have tried two different approaches so far in our preliminary work. So the first approach, we translate natural language into this abstract meaning representations, and then to augmented abstract meaning representation. So the first approach is more focused on how we can get the mapping more accurately. So from natural language, we step-by-step prompting it or fine tuning it to generate more accurate results. in AMR and then we, because AMR has its own limitation and it's not enough for having a good logical representation. So we try to amend the AMR with all those information preserved and so that we can get to the logic representation in the next step. And then, yeah, we have also a different approach, which actually just translate natural language into logical representation and fit it to a logic engine. So far, we have tried these two approaches, but they are not by no means to be isolated. Actually, they can complement each other. But let me. explain it step by step. So this is the first approach that we tried to do the natural language to AML mapping. And this is kind of based on the work from Richard Shin and his collaborators. What they have done in the past is that they also use a LLM like GPT-3, but then they use a LLM to translate the natural language into some domain-specific language, kind of like a meaning representation of the language. With a few tricks, for example, They use a constraint beam search. The constraint is put on the regular beam search and the constraint being basically a function that would check what we have generated already. And it will return a set of tokens that is valid for the completion. So the idea is that they're basically trying to make sure that the generation is in a syntactically valid form in this approach. Yeah. Let me send this in response to. So, oh, also, I forgot to mention that later on in their work, they use the code generation model, like CodeDance, to redo the work. And then they found that the accuracy improved a little bit. So based on the two experiments that they have done, we tried to replicate it in in our own way, but instead of translating natural language into the domain-specific language, we translate it into AML. Here's one example if you're not familiar with it. What we have tried is we also have used some open AIs model like GPT-3.5 or GPT-4 even, later on. And then we kind of do trying to do the same thing with the constrained beam search that the constraint is based on the syntax of the AMR. And then we also try both prompting of the model to both field short learning basically and fine tuning model. Some of the approaches are not available in the OpenAI API. So we have also tried to use different open source models. So here is some preliminary results that we see on the top of the tables. We see the accuracy of the OpenAI models like GPT-4 is actually doing pretty good for simple sentences, but not so well for a little more compressed sentences. In the middle of the table, we see basically CokeGen is an open source Coke generation model from Salesforce. And then we have tried it with different parameters. We are planning to try the CokeGen2 model as well, which is released Last month, I think quite recent. So we are also exploring with the model here. So we have tried to fine tune a. Elama model, which even for the smallest model like here, actually fine tuning on the AMR examples that we have. And we can see that actually the accuracy is pretty good. And it is already better than GPT-4 in a few short learning experiment in the GPT-4, because they don't have the fine-tuning APIs released yet. All right. So here. Right. 

S07 [02:25:04]  : Yeah. So I just want to mention about these data sets that we're using are very small yet. We haven't gone to systematically evaluate on LDC, for example, which is a standard for evaluating AMRs, because we're still in an exploratory space, uh, space, sorry, as Mankin previously mentioned. Uh, but yeah, but the results look promising with fine tuning. Yes. Am I reading that correctly? 

S03 [02:25:26]  : That the 7 billion Lava model out of the box, you got no result, but it's terrible. 

S07 [02:25:32]  : Yes, it's terrible. Um, it doesn't even give correct parsing, valid parsing. And then, I mean, 30 and 65 are already decent. but just fine tuning the 7B gives very good results. And of course, this training set, sorry, these testing sets are not in the training. Just wanted to mention about that. So yeah, that, that's one important avenue of continuation, which we'll talk about later, but fine tuning seems convenient also because UBT, although somebody mentioned before that it's not super expensive, but it's, it feels up. And if we're going to be using it a lot, then it builds up for sure. Um, and also open source models that we can share with people. The next step in this first approach that Manhin was talking about, which was natural language to AMR, comes from the reason that, as Manhin showed, this is AMR. I don't know how much people are familiar with it, but it's basically a graph of sentences that the boy wants to go. So it creates a semantic representation. It doesn't store all the particulars about the sentence, for example, here. It's not distinguishing. If it was the boys want to go, it's the same exact trait. It doesn't specify that. But Stabler I think Oleg himself, yeah, suggested an augmentation of AMR, which is called augmented, AMR means abstract meaning representation. So an augmented version of that, where for some linguistic purposes, it keeps a little bit more of the syntax of the original sentence. So this is how the tree looks in AMR. And just to highlight the differences, well, it's basically preserving the tenses of the verbs that are present in the original sentence, the number of the nouns, so it's adding singular there in that note, and if there are quantifiers in the sentence it adds them as an extra note. So we thought that for reasoning and for some cases, it's important to have this. Of course, quantifiers are important. So we started exploring to use AAMR. Also, first we thought about running again the same models with the code. Then also we thought maybe we can try directly on the LLMs, how they do. And the conversion from AAMR to AAMR could also be done like in a rule basis to know where you start checking the nouns and the verse. But the LLMs do it very well already. So the word alignment was a little hard, but you do it with rules and make sure that you're labeling the right notes. But LLMs are good at it. This is a couple of examples where you have the original AMR. You give it the sentence and the AMR. Of course, you have to explain in a prompt what it has to do because AMR is not so standard. And then it gives you the right tags or something that is acceptable for the purposes that we want, which is to have more of the content of the sentence represented in the graph. 

S03 [02:28:52]  : What did I want to say? 

S07 [02:28:53]  : Yeah, just an example of how the verb tense wouldn't make any difference in AAMR. The AAMRs are exactly the same, but in AAMR you have, I think I highlighted the wrong part. Oh, this is, yeah, sorry. Here it's present and here it's past. Just because it's dot as he thinks it's a link, sorry. And then we also tried more complex sentences. like for every boy there's a girl who calls that boy's dog by some special name with quantifiers and a little bit more complicated the AMR we can produce it also with the neural networks with the LLMs and then the translation to AMR it has some or it has Pablo Su?rez Hern?ndez-Sandoval): includes important information like this singular for girl boy, there are some things that we're not sure if they're actually right like here is changing. Pablo Su?rez Hern?ndez-Sandoval): A role in the Mr it's completely changing it to the quantifier because it part of the idea of amrs is that you have quantifiers explicitly as an old but it's actually replacing. One original name are, I think for, for the representation that we need to understand in our logic system is good, but we, uh, we're still exploring this, right? So it doesn't do a bad job directly with the LLMs, but then, uh, models are improving a lot. That's everybody has been talking. We, we tried with GPT 3.5, um, Okay. Let me go back for a second. What do we want just to remind you is to be able to process in our logic and, you know, in, in BLN and hyperon. So if we could go from natural language to logic directly, it's a, it would be great with 3.5. It wasn't very good, but then we, one day we got the API for you before. So we started trying with that and we realized that it does a very good job. translating a natural language directly to predicate logic. So yeah, they don't do every logic problem perfectly. There are errors, but they're pretty good at translating to logic predicates now. So this is an example where I just give, actually, this is a zero shot situation where I don't even need to explain what order logic predicate logic is. It knows already. So the only animals in this house are cats. And it gives me a, a logic expression and it even gives an explanation. This is like the first tries that we did. And then we, of course. Improve or fine tune the prompt or improve the prompt to give us a format that we can use to, to then ingest in a, in a logic engine. Yeah, so this is the other track that we're trying, which, as McCain said, they can be complemented. And maybe we want, we're not sure here, we're still exploring, but maybe we want to have both representations of AMR and predicate logic separately to maybe get information that's not in the other representation. But then Yeah, in order then to process it with a logic engine, well, we need to integrate it somehow with whatever logic engine we're using. And we have been trying a couple approaches. One is, of course, Hyper-ON OpenCog PLM, which is a... What does it stand for? There you go. Sorry. My team is working on that. But yeah, this is still a work in progress because Hyperion is still evolving and we're working on getting the representation in there. In the meantime, just to see if we actually got good results, we're using PyProver, which is a prover, a pure prover available in Python. And we built a little script that shows how you can go from natural language to solving some logic problems, which is what I'm going to show here. Just with a few simple examples, as Maheen already mentioned, it's similar to what, or it's in an earlier stage to what Patrick showed earlier, but well, that's where we are at. Let me find, where is it? 

S08 [02:33:35]  : Here. 

S07 [02:33:36]  : Okay, so first, the problem that we had, that my team showed in the beginning, can you see this? No, not sure. I don't know how to increase the size. Yeah, it's pretty small. Sorry, maybe he can make it work. But in the meantime, I'm just going to say it's a, it's the same problem that we had in the beginning that GPT for fails at by making a wrong conclusion. And we feed the premises and the question. And actually you can make the question like that is in natural language. It doesn't have to be like in theorem brewer, like where is red and then. Is that better? It's okay, we can run this. What is it now? All right. 

S00 [02:35:12]  : Sorry, a little problem here. 

S07 [02:35:18]  : Okay, so I run this and then it sends the sentences to inject them into a prompt and then sends it to. large-language model, and we will get a result in a second. And we can also see what kind of propositions it made, and then we convert them to PyProver. PyProver easily solves an easy problem like this, no? In this case, well, the answer is that the premises do not prove that the bearer is red, so it will just say false in a second. Here the bottleneck is always the calls to GPT-4, which of course would be alleviated if we have our own model running locally or something, our own fine-tuned model. So here, the answer should be that we cannot prove from these premises that the bear is red. You get it there? Another interesting thing is that, well, of course, or one of the most appealing parts of these large Nyquist models is that they can Actually, they can interpret or resolve an AFRA. An AFRA resolution is handled here. I say, Anne is high, she is tall, and anyone who is tall is also red. It knows that she refers to Anne immediately, which other systems struggle with. So I will also run it. I didn't show the representations created in the last step, but I will show them here. Yeah, a couple seconds. 

S04 [02:37:03]  : It's taking a little longer. 

S07 [02:37:13]  : There you go. So here we see that it converts the 1st sentence into a. Adequate budget representation, 2nd, 1, 3rd, 1, and then the question as well. And this is a random. Let's go with the. around the dog situation, which is more complex. That's why I wanted to show it. It's the one that we showed in AMR. For every girl and every cute name, there is a boy who calls that girl's dog by that name. And Annie is a girl, and Fido is a cute name. So it represents that adequately, even if it's like more complex. And then the question is, if there is a boy who calls Anne's dog Fido, and the answer is true from this premises. And we could try this, but it's going to be the same. And the enough for a resolution, just because I promised it. 

S03 [02:38:11]  : Yeah. 

S07 [02:38:14]  : Um, same for worries that it's going to actually understand that. So, Hmm. Yeah, here the question is if Anna is red or not. And it correctly understands here in the translation that when I say she's tall, it knows that I'm talking about Anne. Right. Of course, you can go more complex than this, and we are evolving the system to handle all of that. I think that goes into the next steps. Let me go back to my presentation. Yeah. So as I, as we said, like this exploratory, the next step is to actually integrate correctly with open go hyper open hyper, and be able to use BLM there. And, uh, of the capabilities that it has, um, to also handle fuzzy logic, like what we saw with, with, uh, NARS and stuff like that. Um, of course we, yeah, we want to. Pablo Su?rez Hern?ndez-Sandoval): able to obtain knowledge from a large data set of common sense or not comes with like world knowledge like wikipedia or the someone told you were evaluating and create a big corpus of knowledge for for to use with hyper. Pablo Su?rez Hern?ndez-Sandoval): or evaluate the performance systematically when when we are at that stage. And also, depending on the results with that, we would see if it's important to improve the mapping. If the mapping with only logic is not enough, we could use the syntactic representation that we have in AMRs and AMRs to enhance the understanding of our system. Yeah, and there's, as Mankin mentioned, there is potential of using all of this chain of logic reasoning to improve also PLN itself. And I think we can invite Ben to talk a little bit more about those future steps as well. 

S01 [02:40:21]  : Yeah, so thanks for explaining sort of the work we've been playing with. And we saw earlier, Patrick and colleagues have been playing with similar things. And I know from discussion with Stephen Wolfram at Wolfram Alpha Team, they've been playing with similar things also. And it seems with quite modest effort, one can get decently high accuracy on translating natural language into various formal languages. This is a context where decently high accuracy on its own is probably not good enough. It's good enough for demonstration and experimenting. It's not good enough for real-world applications, because let's say 5% of your semantic relations are wrong. that's still really bad. If you're trying to do biomedical discovery or even answering people's questions about everyday stuff, pretty much you're facing a similar issue to what is faced with CHAT-GPT, where it's just making up some stuff and then other stuff is real. Now, that One of the hopes here, though, is once we've loaded the knowledge into the atom space, or into whatever logic representation you're using, you can use reasoning to cross-compare things, right? And then that can eliminate the errors, because many of the hallucinations or mistakes that the LLM makes it actually, in a sense, knows that that thing was wrong, right? Like, it has information contradicting that, that it will say in response to some other question, right? So then, if you've taken a whole bunch of productions and loaded them in, and then done reasoning to, you know, derive some conclusions, do belief revision and whatnot, then, I mean, perhaps you can make do with some level of error, and how much error there can be and have that cleanup work is an empirical question, right? Like if it's half wrong, it probably doesn't work. If it's 1% wrong, certainly you can clean up that, right? So it's almost like an ablation thing that you're talking about, like what percent of nonsense can you afford in your logical knowledge base and have uncertain reasoning, sort of get rid of the nonsense by cross-comparing things. And we haven't run that experiment yet, but we could run that experiment now, right? It's just doing a bit more work will reduce the amount of nonsense bit by bit. So it's sort of, we're going to keep banging on this until it seems we've reached diminishing returns in terms of eliminating nonsense and then capturing relationships. Once you reach a point of diminishing returns in eliminating nonsense and capturing true relationships, then we say, well, okay, that's about what we can do with current easy-to-use technology. Now let's see what the reasoning engine can do to clean everything up. I think we're We're maybe a month or two away from getting to that point, I would imagine. I mean, with the caveat that people keep releasing new language models to try. So in a way, you may reach a point of satiation, but we're still improving based on whatever new things come out. But let me say a little bit on what I think we can get out of this and what we can do in this direction that's beyond extracting language from logic. I mean, merely extracting language from logic into a reasoning system is very good, right? Because, I mean, Manhit and I, along with my wife, Ray Ting, and a bunch of others, put in a bunch of work over several years working on extracting language into logic using a combination of statistical NLP tools and hand-coded rule bases of various sorts. I mean, that didn't work as well as these crude first experiments, actually. I mean, in some regards, it may have worked better on particular things where we put a lot of attention. On the whole, it was not as good, right? I mean, even this level of performance is rather cool. And just getting all that knowledge in a knowledge base itself is cool, right? So, I mean, thinking about, for example, bioinformatics and biomedical reasoning as an application area. So, we've got a huge amount of bioontologies integrated in OpenCog Atomspace. We can process genomic and medical datasets and recognize patterns in those and extract the patterns in the datasets into the Atomspace Knowledge Draft. But then, if you can take the semantic relations from PubMed abstracts or the papers, in some measure, put those in the atmosphere, then you've got a lot more information to use. And we did this in a much cruder way a long time ago, using cruder semantic relation extraction tools, which this stuff is already better than, right? So, there's a lot of mileage to be gotten there in all sorts of application areas, including biomedicine as just one. On the other hand, there's also more profound things you can do. One interesting thing you can do is build mappings between the internal state of the transformer and the internal state of the open cog atom space where the reasoning engine is running. This can be done both statically and dynamically. static way to explain first. So we've done work previously, it was Man Hinlong who did that work, Debbie Dong and some others, making vectorial projections of the atoms inside the M-space. So you can take each node or link, subgraph, however you want to set up, project it into a vector, say a 50-dimensional vector. You can do that using DeepWalk. We did it using some PLM preprocessing followed by kernel. You can use a lot of methods. One could do something similar projecting to hypervectors, as we discussed in Rachel's workshop this morning. For this particular purpose, projecting into plain old dense vectors is the more straightforward thing. You projected each node into a vector. You have the internal attention vectors you can extract from the transformer neural net. So, you can then attempt to put probes into the transformer. figure out what activation is having the transformer when it is producing things about a table, a chair, a house, and whatnot. Then what you can do, you can train a matrix to map from the embedding vectors of the nodes in OpenCog with the vectors that you extract measuring the state of the transformer. This should be quite straightforward. I mean, if you go back a while, some folks they published a paper extracting a matrix mapping the internal state of the transformer into a parse tree. So, they showed you can extract this conventional syntactic parse tree from the state of the transformer. So, I mean, you can extract a parse tree from it. Most likely, you can map it into the nodes in the semantic network, and not only for individual words, but for atomic-smatic relationships, similar to the links in a parse tree. If you can do that, you have something a little more interesting. Instead of just taking the output of the transformer into into atom space. You can activate a whole bunch of nodes inside the atom space in accordance with the nodes that are activated inside the transformer. Why would you want to do that? Well, if you're asking a question of the atom space, so the question is first posed to the transformer. Translate the question to logic, fine, PLN can answer the question. But if you're then simulating the nodes in the atom space, that are transformed by the transfer matrix from the internal nodes in the transformer. Then the attention allocation in the atom space will highlight, for the reasoning engine answering the question, it'll highlight the things that the transformer thought were relevant, right? So you're not just doing inference to answer the question, you're doing inference control and guiding the inference engine to answer the question based on the things that were sort of laterally referenced by the transformer in interpreting, in processing the query, right? So, that's the static mapping between the two. Now, the other thing, which is what really got me excited about this actually is, so you can ask the transformer to do reasoning. You can do chain of thought, and you can then do even fancier things in typical chain of thought when you try to get to come up with an in-depth logical justification for each step made in the chain of thought according to some menu of inference rules. So you can tell the LLM, like, well, try deduction, try induction, try abduction, try generalization. Can you justify this inference that we did according to these inference rules? And it can do a fair job of it. So you can actually milk a fairly detailed inference trajectory out of the LLM. Well, once you've done that, that inference trajectory is natural language. You can translate each of these steps in the inference trajectory into natural language, right? Then you can map each of those in logic form into the atom space. Now, this may or may not be a valid PLN proof trajectory, but it's a set of waypoints, right? So it's like here's premise, next step, next step, next step, next step. You then ask the PLN inference engine to make a proof that sticks close to that series of waypoints, right? And it may follow exactly between them, and maybe the LLM was a little bit off, but there's probably something close to that, right? And this is a mode of PLN inference control I experimented with a long time ago in an earlier version of OpenCog. was cool, but I didn't then have as good a source of the intermediate waypoints to sort of give the path to follow, which is something that's new with the LLM technology. But you could also go the other direction. Once you found a nice proof in PLN, You could then just feed that proof back into the language model. You could try translating it back into natural language, or you could just feed it back in PLN form, because the LLM has been fine-tuned for translation anyway in the early stage of this process, and you're then teaching the LLM as much as it can figure it out. I mean, you're teaching the LNN to emulate what PLN has done, which I don't think it's going to be as smart as a real inference engine ever at doing inference. But if you can make its justifications a little cleverer, right, then you're improving this whole pipeline. Now, that's final step to all this, which is going to be a few months before we get there, perhaps. Once you're doing a lot of reasoning on natural language data in OpenCog, what we've done before is we've taken all the inferences that it's done, and we've saved the whole record of each inference in an auxiliary atom space. So, you have this big distributed atom space of all the inferences a thing has ever done, right? So, what we do then we've traditionally run NILS PatternMinder. Well, now it's Hedris PatternMinder. We've trained all of our PatternMinder on this large atom space to find which patterns characterize successful inferences versus unsuccessful inferences, right? So, that's nice, and the PatternMinder is very good in some ways. But what we could now also do is we could train a GPT model on this auxiliary atom space, because in the end, this is just a bunch of time series. It's like, this inference step, this inference step, this inference step, this inference step. You have a huge number of time series. You can train a transformer on that. The transformer can then guide the pattern model, which we're going to look for more advanced patterns. You can then use this transformer, which is trained on the inference history admin space, you can use that to help guide inference control, right? Because what is guiding inference control? You're like, well, okay, we have this history of five inference steps. We want to see what, this is six, what should we choose to continue? And we have the context, which is all the other inferences have been tried. in that whole inference process, right? So that's exactly the sort of problem that a generative sequence prediction model like GPT is good at, right? And then, of course, we can recurse that further. Like, you can apply PLN to induce and abduce from the set of inference histories also being guided by that transformer trained on the inference histories. I think the probabilistic foundation of PLN, while we haven't talked about much here, is an asset here because the transformer is a probabilistic model. I mean, the transformer is precisely making probabilistic predictions, right? So then using PLN to be guided by and guide this transformer, which is also making probabilistic predictions. You have a lot of synergy there, and that's cached out in various ways. You can use statistical sampling to guide both of these, because this is… One other point that comes up here is, in order to connect Ultimately, to connect the transformer with PLN, we would like transformers to output confidence values as well as merely probability values for predictions. Because PLN, like NARA, deals with both strengths and confidence. In PLN, we can interpret them as interval widths of a second-order distribution. But transformers right now not do that. This leads to the following. In the end, we're going to need to retrain our own transformers for maximal compatibility with our probabilistic logic framework. Everything I described up to this point we can't do with existing transformer models and existing OpenCog logic engine, which is nice because it costs a lot of money to train these transformers. On the other hand, there are various reasons you would ultimately like to train a custom transformer model to suit your AGI framework. The first and simplest reason is you want confidence. You can get confidence in a hacky way by paraphrase each input to the LLM 20 ways and see the variance of the outputs that you get from the paraphrased inputs, which sort of works, but it's kind of annoying. You could also modify the transformer training code and go through the whole learning process with second order probabilistic truth values, right, which is mathematically quite straightforward, because you have the same algebra on the second-order truth values as you do on first-order probabilities, right? So there's no intellectual obstacle to overcome in just training a transformer to output second-order probabilistic predictions instead of first-order. It's going to take one to two orders of magnitude more time than training a transformer does now, which is a lot of electricity, which is annoying. On the other hand, that's what happens. People are coming out with papers telling you how to train a transformer 10 times as fast. So, naturally, we will then come up with a way to make it 10 times as long by doing something fancier, like putting second-order probabilities inside. We're also working a bit with Alex Arorbia from RIT, who has a method of training neural nets using predictive coding, which in some cases seems to work much better than backpropagation, right? So, then if you can use that, to make the transformer smaller and smarter, then that may make it easier to do things like train it using second-order probabilities, as well as having other advantages, like maybe letting it learn semantic latent variables that improve the mapping into the internals of OpenCog and so on. So, there's more, but that's more than enough for now, anyway. So, I mean, I think that what we're doing now relatively simple and early stage, but it's just going so much faster than comparable things that we did a decade ago, which is quite cool. So I think we're actually going to have gotten a decent way through some of these steps I've just described now by the AGI conference next year, for example. 

S02 [02:57:30]  : Thank you very much. 

S00 [02:57:31]  : All right. 

S08 [02:57:36]  : Thank you very much, Ben. Thanks, Andres. Thanks, Manhin. We're actually on a tight schedule. We're going to get kicked out of the space. So if there's time for one quick question, I'd love to take that. And we'll move right along. 

S01 [02:57:50]  : We've resolved all possible problems. 

S08 [02:57:53]  : Collapsed into a solution. Great, so our last presenter of the day, last presenter of the conference, no pressures, is Nick Mikhailovsky, Multiscale Phenomena in Languages and Language Models. So welcome, and thank you very much. 

S05 [02:58:12]  : Oh, can you see and hear me? 

S08 [02:58:15]  : It sounds great, looks great. 

S05 [02:58:18]  : Okay. Thank you. I am Mikhailovsky and we'll talk today about multi-scale phenomena in languages and language models. Our respected previous speaker have been referring to transformers as state-of-the-art language models, and that's true, they are state-of-the-art, but a lot of what I will be speaking off about is that transformers are not that good models for the language as we currently see, and they don't really reflect a lot of important features of the language. Basically, we are relying in a very general sense on the idea that every good regulator or system must be a model of that system. And to start with, we'll talk about multiscale structure of the texts. Basically, that's super obvious. Multiscale structure of the texts is present everywhere. And if you look at the long texts like Tolstoy's War and Peace, we see at least seven layers of multiscale structures. At the very top layer, there's a book, one piece, which is split in the four books. Every book is split in parts, parts are split in chapters, chapters in paragraph. than in sentences, sentences in words, in characters. And there are at least seven layers of multiscale structure, but probably much more because chapters are a lot of paragraphs and paragraphs can be grouped into more layers and probably chapters may be grouped in more layers. So seven layers of multiscale structure of war and peace is like minimum number. And actually that reflects the way human thinks. For example, when human tends to write a long text, our thinking is also multi-layered, multi-layered. Uh, not after regressional. So we think probably about core concept, then about writing structure, then about wording and phrases. But, uh, probably it's more, even more complex than that, because we start with some sentences that go up to structure, then down to sentences and go up and down. Like we go with a very generalized parse tree. So let's take a look at the models of the language. Actually, in our consideration, we'll need most of the models of the language that are currently existing. Actually, there are three established types of models of the languages. that are well established, that's generative formal grammars, distributional semantics, and probabilistic language models that are traditionally author regressed. And there's a newcomer diffusion probabilistic language models that are not author regressed. So let's take a look at each of these classes. Generative grammars were proposed by Noam Chomsky in 50s. And that's a set of rules that allows one to generate grammatical sentence and not generate any ungrammatical ones. Typically, they consist of a finite set of production rules. And these production rules allow us to build parse trees. Chomsky grammars constitute a hierarchy and grammar types go from regular, which everybody who program know as regular expressions. That's because regular expressions are regular programming languages, regular languages. Then there are context-free grammars, which are equivalent in a sense to lean grammars that are popular in here at AGI conference, then there are context sensitive grammars and recursively enumerable grammars, which are equivalent to Turing machine. But what's important for us that context free grammars, which are the cornerstone of modern computer science as whenever we program, we use context-free grammars as parsers of the software we write. Then in context-free grammar, each production rule is from A, which is a single non-terminal symbol to alpha, which is a string of terminals end on. Non-terminals, they define parse tree, which are in a sense, a good match for hierarchical structure of human text on a grand level. But context-free grammars typically work on micro level of sentences and sentence parts. Then there's a distributional hypothesis, which was first introduced by Harris in 1954 and popularized in the form, a word is characterized by the company it keeps by Firth. So the idea is that we collect distributional information in say, multi-dimensional vectors and then define similarity in terms of some metric. For example, Euclidean distance or angle between the vectors. And there were a lot of generations of context, a lot of generation of distributional semantics. The very first generation of distributional semantics based on manual features, then there were such distributional semantics as Latin semantic indents. The second generation of statistical distributional semantics is characterized by Glove and Vortovec, which were popular like 10 years ago and until February 2017. And that includes word analogy and the latest generation of distributional semantics include contextual vectors like BERT, which probably started that. But we will be mostly concerned in this work with embeddings for words, because that's what we'll want to analyze. Now, probabilistic language models were probably introduced even earlier by Markov in 1913, who have analyzed the text of Russian poem, Evgenia Nikin. And that was actually the first application of Markov chains because before that Markov chains were a purely theoretical concept. So probabilistic language models imply that random sources emit tokens with certain probabilities and a parameter of a set of sources is determined so that probability of a real text is higher than a probability of unreal or bad text. And that was actually introduced by Friedrich Jellinck's group at IBM in 80s. And since then mostly existed as author regressive probabilistic language models first introduced as N-gram language models, but uh now all the large language models that we work with now are also author regress probabilistic language models and they estimate uh probability of a sequence of tokens using a chain rule that is factorize the probability of a sequence of tokens into conditional probabilities of tokens, given the previous ones. And typically, these language models introduce Markovian assumption that the probability depends only on the fixed limited number of predecessors. Now there are diffusion models, which are super successful in image generation, but not yet super successful or super prominent in language analysis. The core idea of diffusion models is also to construct a Markov chain, but a Markov chain, not in author regressive sequence space, but in space of multi-scale structure of, for example, images. So the idea that we first corrupt the image, adding Gaussian noise to the image, according to a certain schedule, And then teach neural network model to reconstruct the image back from the corrupted noisy image. And it's very important and very interesting that by the property of Gaussian noise, the more noise we add, the more we filter out high frequencies, for example, in an image. So adding a lot of Gaussian noise, just only leaves us very low frequency, high level features. With less Gaussian noise, we see more details and less high frequency is filtered out. And with no Gaussian noise, we see high frequency features such as details of a structure of a human eye or even hairs. So diffusion models are naturally multi-scale aligned. And very interesting feature of diffusion models is that the schedule that defines diffusion is a schedule that defines the granularity or attention to certain levels of features. And actually a lot of success of diffusion models lies in the fact that the first address and spend a lot of effort on reconstruction and reconstructing low frequency general details of the image and only then refine that to high frequency. Very recently, in the past two years, there were more models of diffusion for discrete text. But the problem with the text is indeed that unlike the images that are in a continuous space, text lives in discrete space. And thus, uh, we work with. Uh, text, we need to somehow go from, uh, discrete space to a continuous space. And that's, uh, where a lot of effort and problems lies and, uh, different works and different teams resolve this problem differently. But still, what's important for us that other regressive language models don't have anything to do with a multi-scale structure, unlike generative grammars, especially context-free grammars and diffusion models. And distributional semantics also don't have anything to do with multi-scale. Now let's look at a different part of things at our research instrument, author correlations. We'll talk a lot about author correlations and author correlation decay laws. And the question is why all these author correlations and author correlations decay laws matters. Let's introduce author correlations using distributional semantics properly. So if we have a text and if we have assigned a vector to each word in a text or each token in a text, we have a sequence of vectors and we can build an author correlation function over these sequence of vectors. by averaging the correlations between vectors that have certain distance between them in this sequence. So the other correlation function C of tau is the average similarity between vectors as a function of the lag between these vectors. Now, there are important properties that actually come from solid state physics and that are very similar to solid state physics laws that describe matter in certain states. So if there's a Markovian process, the other correlations decay law will be exponential. And that's easy to figure out. If we look at the Markov matrix, the Markov matrix will have the same eigenvalue or singular value, the largest eigenvalue of a Markov matrix will be equal to one. but all the other eigenvalues will be less than one. And most of the arbitrary vectors, if we multiply by, by this metrics will, decrease exponentially as the second largest eigenvalue. There may be cases that are degenerate, if the metric is irreducible or periodic, the mutual information or other correlations can decay to a constant But what's important that no Markov process can produce power-law decay. On the other hand, there exists probabilistic context-free grammars that generate power-law decay of mutual information or of the correlation. That's also easy to figure out if we have a tree The tree distance in, for example, if we have a binary tree, as is often the case with context-free grammars, we'll have the structured distance through the tree. proportional to the logarithm of the distance. And thus we will get the author creation decay that's also the power law. What we see with author regressive large language models is that they exhibit Markovian behavior. For example, text generation, whenever we, whatever sampling we use, be it beam search, case sampling, piece sampling, whatever, They degenerate into short sequences of a few words that repeat. That's actually a degeneration that's very similar to Markov model degeneration. And we haven't proved such kind of a theory, but I believe this type of a theorem can be proven. Of course, the larger the language models are, the bigger the boundaries of long texts, but that doesn't solve the problem of degeneration. Actually, the fact that most language models currently in production use are intended for work in the dialogue, which means that's generating short texts and waiting for another prompt. I believe that's in a large, in a large sense, a way to avoid this degeneration problem that's well known to practitioners. Actually, there was a great work of DeepMind I think team on neural networks and Chomsky hierarchy that worked, that tested several neural network architectures on artificial language learning. And they have found that transformers don't even completely able to represent regular languages. RNNs are able to represent regular languages, LSTMs are able to represent a little bit more than regular language, and as we could imagine, stack RNN is able to represent context free grammar based languages. And to represent a context sensitive grammar, we need type RNN. So if the natural language exhibits parallel correlations decay, we can do better than author regressive language models. And that's what's on my second slide, because we want a model to be, uh, we want a regulator on the language, basically generating language means regulating and to have a good regulator, we need to have a good model of a language. So let's look at how a real human generated language behaves in terms of autocorrelations. And we can set up a few research questions. Can we say that author correlations in text decay according to a power law? Can we reject the hypothesis of exponential decay of correlations, author correlations in long text? Do the loss or decay depend on language of the text? Over what range of distances does the decay of author correlations follow the power law? And do author correlations in language model generated text differ in any way from literary text? To answer these questions, we have analyzed quite a few texts in different languages, basically long literary and philosophical texts in five European languages, English, Russian, German, French, and Spanish. And we have found that the author correlations decay is indeed following the power law. And following the power law means that in log-log coordinates, the author correlations graph looks like a straight line, which it does after some peak that happens between one or 10 words of distance. If the other correlations would be exponential, we would see the straight line in the top right graph where the ordinate is logarithmic, but the distance is linear. Actually, if we shuffle the words in a text randomly, the off correlations graph will be also random and we will not see any decay. We can measure the accuracy of approximation of these graphs by power law and by exponential law. And we see that independently of the language, independently of the text, the power law approximation is typically much better than exponential law. And besides the using GloVe as a distributional semantics, we also use back of words and what similar results. The only difference, the only exception is Iliad in French. And, but there's an obvious explanation. You can see that the approximation of Iliad in French is much worse than other besides it's better by exponential law by then the by power law. And we, if we look at the graph, we'll see that the author correlations on distance one is positive, but very small. And it breaks approximation and breaks the dominance of power law. We can also look at the efficiency of the best fit of a certain text in different languages. And we'll see that in most cases, it only slightly depends on the language of the text. So we can see that the, decay, the author correlations decay law is a property of the text, maybe a property of the structure of the text or the property of human thinking about this text, not the property of the language. We can also look at how far does this power law extend in human texts. So for this example is Plutus Republic, and the blue is where the power law is the best approximation. Actually, the only meaningful part is upper right triangle, but anyway, this means that on most distances power law is the best approximation to the decay law of the correlations. And actually, natural probabilistic structure of the Texts on short distances allows us to see declines from the power law. So how about the author correlation flows in large language models generated text? That's a work in progress, but for some language models, we see that of the correlations law are exponential. For some, we sometimes see more likely power law, but we believe that for most sampling approaches and for most parameters, hyperparameters of large language models, of the correlations decay law for texts generated by large language models is exponential. And this is also on whenever we can definitely say that it's exponential and we can't reject the power log hypothesis, the Quantity of difference from human text is very large and very noticeable. So the author correlations in automatically generated texts are declining significantly lower and are significantly larger than in human written texts. Basically, that means there is much less information in these texts. And these texts all suck. Whenever we use large language models, an example here only ties GPT-2, but we have recently done experiments with much larger language models. Whenever you ask a large language models to generate a long text, The long text sucks. For long text processing and for emulating human thinking, we may need architectures different from author regressive ones. And a lot of questions remain unanswered. Some of these questions relate to different models, such as recent RWKV model, which extends RNNs to transformer space. Diffusion models were not ever tested on long-tax generation and so on. There's a lot of questions about why the author correlations behavior is such as we have observed, for example, what is the source of this peak in author correlations? What are the relationships between different statistical characteristics of text and so on? There's a lot of linguistics questions that are yet unanswered. and so on. But we believe answering these questions, uh, will allow us to engineer more realistic communication tools for the neural networks because, uh, computer vision have provide perception layer for, uh, machine learning models. That's kind of AI Language generation allows a communication, but we'll need some kind of thinking or model building that will be integrated in a way with both perception and communication. And we need to somehow reflect the structure of the work, which is multi-scale and hierarchical to be able to represent the world in these models. Basically, that's it. We will be happy to answer your questions, if any. 

S08 [03:30:02]  : That was absolutely perfect. We might be able to squeeze one quick question, but we're being asked to leave. So one quick question from in here. All right, I think we're great. That was awesome. I learned a bunch of great distinctions. 

S01 [03:30:16]  : I have a non-quick question. 

S08 [03:30:18]  : Non-quick question. 

S01 [03:30:21]  : If the autoregressive models are not going to be good enough for dealing with that with long texts, then what does he suggest? 

S05 [03:30:30]  : Well, I believe we need to go for some kind of model that has hierarchical and multi-scale structure. I think there are at least three types of models that can cope with that. First, there are models that directly model tree-like structures like context-free grammars or link grammars. And there may be a way to build language models that directly address this thing in directly build text as a tree. The second viable approach is to use some kind of memory, which is typical when we go from from FindAlphaMeta to context-free grammars, adding a stack to context-dependent grammar to Turing machine, which has a infinite tape. So adding a right type of memory to a language model may work in building some kind of hierarchical structure. And finally, there are diffusion models that also directly address multiscale a structure of things. And there may be a way to work with diffusion models as language models to, to address this multi-scale and multi-structure nature of things that are around us, let's say. 

S08 [03:32:41]  : Great, thank you so much, Nikolai. Did you get your answer, Ben? Yeah, yeah. 

S01 [03:32:48]  : Awesome. Those are all doable things. 

S08 [03:32:52]  : Wonderful. All right, thank you very, very much, everybody. Thanks for attending AGI 23 right to the end, giving all these great presentations, great audience, great questions, great participation online and live. And we look forward to seeing all of you and about 400 of your friends next year, friends and colleagues. Thank you very much. And then as we exit this space, if everybody could grab cups, napkins, anything you see around, we'll leave it cleaner than we found it. There's still some proceedings left. I'm not sure. There might be this box right here. If not, we'll let you know. So grab your proceedings. 






https://agirussia.org/
Мы ведем группы и организуем семинары русскоязычного сообщества разработчиков систем AGI (Artificial General Intelligence или Общий Искусственный Интеллект) или Strong AI (Сильный Искусственный Интеллект), а также - являющийся их частным случаем HLAI (Human-Level Artificial Intelligence или Искусственный Интеллект Человеческого Уровня).

Группы:
https://t.me/agirussianews (новостной канал)
https://t.me/agirussia (основная)
https://t.me/agiterms (вопросы терминологии)
https://t.me/agibots (разговорный интеллект)
https://t.me/agifintech (финансовые технологии)
https://t.me/collectivei (коллективный интеллект)
https://vk.com/agirussia
https://www.facebook.com/groups/agirussia (основная)
https://www.facebook.com/groups/socialintelligence (коллективный интеллект)
https://groups.google.com/g/agirussia

Онлайн-семинары идут по четвергам, в 18:00 по Московскому времени. Продолжительность два часа, обычно это либо доклад на один-полтора часа и последующее обсуждение на полчаса-час либо круглый стол с регламентом на усмотрение модератора дискуссии. Технические средства проведения, регламент и модерацию обычно обеспечивает инициатор конкретного семинара либо спикер и его коллеги.

Регистрация на семинары (внизу страницы):
https://aigents.timepad.ru/event/1412596

Программа следующих семинаров:
https://agirussia.org/workshops.html
